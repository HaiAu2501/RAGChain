from src.state import OverallState, QueryResult
from typing import Dict, Any, List, Tuple
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from langchain.schema import Document


# Global variables (will be set in main.py)
VECTOR_DB = None
BM25_INDEX = None
N_RESULTS = None
SEARCH_WRAPPER = None
USE_HYBRID = None
DENSE_WEIGHT = None
SPARSE_WEIGHT = None
FUSION_METHOD = None
RRF_K = None


def initialize_observation(cfg, vectordb, bm25_index=None):
    """Initialize observation node with configuration, vector database and BM25 index"""
    global VECTOR_DB, BM25_INDEX, N_RESULTS, SEARCH_WRAPPER
    global USE_HYBRID, DENSE_WEIGHT, SPARSE_WEIGHT, FUSION_METHOD, RRF_K
    
    VECTOR_DB = vectordb
    BM25_INDEX = bm25_index
    N_RESULTS = cfg.tree.hyperparams.n_results
    
    # Initialize DuckDuckGo search with rate limit protection
    try:
        import random
        import time
        
        # Add small random delay to avoid synchronized requests
        time.sleep(random.uniform(0.5, 2.0))
        
        # Try with minimal settings first to avoid rate limits
        SEARCH_WRAPPER = DuckDuckGoSearchAPIWrapper(
            max_results=min(N_RESULTS, 3),  # Limit results to reduce load
            backend="lite"  # Use lighter backend
        )
        
        print("‚úÖ ƒê√£ kh·ªüi t·∫°o DuckDuckGo search wrapper (ch·∫ø ƒë·ªô lite)")
        print("üí° Tip: ƒê·ªÉ tr√°nh rate limit, h·ªá th·ªëng s·∫Ω:")
        print("   - S·ª≠ d·ª•ng backend lite")
        print("   - Gi·ªõi h·∫°n s·ªë k·∫øt qu·∫£ web")
        print("   - C√≥ delay ng·∫´u nhi√™n gi·ªØa c√°c requests")
        print("   - ∆Øu ti√™n c∆° s·ªü d·ªØ li·ªáu n·ªôi b·ªô")
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói kh·ªüi t·∫°o DuckDuckGo search wrapper: {e}")
        print("‚ö†Ô∏è T√¨m ki·∫øm web c√≥ th·ªÉ kh√¥ng ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh")
        try:
            # Fallback to basic initialization
            SEARCH_WRAPPER = DuckDuckGoSearchAPIWrapper()
            print("üîÑ ƒê√£ fallback sang c·∫•u h√¨nh c∆° b·∫£n")
        except:
            SEARCH_WRAPPER = None
            print("‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o web search, ch·ªâ s·ª≠ d·ª•ng c∆° s·ªü d·ªØ li·ªáu")
    
    # Hybrid retrieval settings
    USE_HYBRID = cfg.tree.retrieval.use_hybrid
    DENSE_WEIGHT = cfg.tree.retrieval.dense_weight
    SPARSE_WEIGHT = cfg.tree.retrieval.sparse_weight
    FUSION_METHOD = cfg.tree.retrieval.fusion_method
    RRF_K = cfg.tree.retrieval.rrf_k


def reciprocal_rank_fusion(dense_results: List[Document], sparse_results: List[Dict], k: int = 60) -> List[Document]:
    """Combine dense and sparse results using Reciprocal Rank Fusion"""
    score_dict = {}
    doc_dict = {}
    
    # Process dense results (vector similarity search)
    for rank, doc in enumerate(dense_results):
        # Create unique document ID based on content hash
        doc_id = str(hash(doc.page_content))
        score_dict[doc_id] = score_dict.get(doc_id, 0) + 1 / (k + rank + 1)
        doc_dict[doc_id] = doc
    
    # Process sparse results (BM25)
    for rank, result in enumerate(sparse_results):
        doc = result['document']
        doc_id = str(hash(doc.page_content))
        score_dict[doc_id] = score_dict.get(doc_id, 0) + 1 / (k + rank + 1)
        doc_dict[doc_id] = doc
    
    # Sort by combined score and return documents
    sorted_items = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)
    
    # Return top N_RESULTS documents
    result_docs = []
    for doc_id, score in sorted_items[:N_RESULTS]:
        doc = doc_dict[doc_id]
        # Add fusion score to metadata
        doc.metadata['fusion_score'] = score
        result_docs.append(doc)
    
    return result_docs


def weighted_fusion(dense_results: List[Document], sparse_results: List[Dict], 
                   dense_weight: float, sparse_weight: float) -> List[Document]:
    """Combine dense and sparse results using weighted scoring"""
    score_dict = {}
    doc_dict = {}
    
    # Normalize dense scores (similarity scores are already 0-1)
    max_dense_score = 1.0
    for rank, doc in enumerate(dense_results):
        doc_id = str(hash(doc.page_content))
        # Use inverse rank as score proxy (higher rank = lower score)
        dense_score = (len(dense_results) - rank) / len(dense_results)
        score_dict[doc_id] = dense_weight * dense_score
        doc_dict[doc_id] = doc
    
    # Normalize sparse scores
    if sparse_results:
        max_sparse_score = max(result['score'] for result in sparse_results)
        if max_sparse_score > 0:
            for result in sparse_results:
                doc = result['document']
                doc_id = str(hash(doc.page_content))
                sparse_score = result['score'] / max_sparse_score
                score_dict[doc_id] = score_dict.get(doc_id, 0) + sparse_weight * sparse_score
                doc_dict[doc_id] = doc
    
    # Sort by combined score and return documents
    sorted_items = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)
    
    # Return top N_RESULTS documents
    result_docs = []
    for doc_id, score in sorted_items[:N_RESULTS]:
        doc = doc_dict[doc_id]
        # Add fusion score to metadata
        doc.metadata['fusion_score'] = score
        result_docs.append(doc)
    
    return result_docs


def parallel_fusion(dense_results: List[Document], sparse_results: List[Dict]) -> List[Document]:
    """Simple parallel fusion - take top results from both methods"""
    result_docs = []
    seen_docs = set()
    
    # Take half from dense results
    dense_count = N_RESULTS // 2
    for doc in dense_results[:dense_count]:
        doc_id = str(hash(doc.page_content))
        if doc_id not in seen_docs:
            doc.metadata['retrieval_method'] = 'dense'
            result_docs.append(doc)
            seen_docs.add(doc_id)
    
    # Take remaining from sparse results
    sparse_count = N_RESULTS - len(result_docs)
    for result in sparse_results[:sparse_count * 2]:  # Check more to fill remaining slots
        doc = result['document']
        doc_id = str(hash(doc.page_content))
        if doc_id not in seen_docs and len(result_docs) < N_RESULTS:
            doc.metadata['retrieval_method'] = 'sparse'
            doc.metadata['bm25_score'] = result['score']
            result_docs.append(doc)
            seen_docs.add(doc_id)
    
    return result_docs


def search_vector_database_hybrid(query: str) -> List[str]:
    """Hybrid search combining dense and sparse retrieval"""
    try:
        if VECTOR_DB is None:
            return ["C∆° s·ªü d·ªØ li·ªáu vector kh√¥ng kh·∫£ d·ª•ng"]
        
        # Always perform dense retrieval
        dense_results = VECTOR_DB.similarity_search(query, k=N_RESULTS)
        
        # Perform sparse retrieval if hybrid is enabled and BM25 index is available
        sparse_results = []
        if USE_HYBRID and BM25_INDEX and BM25_INDEX.bm25_index:
            sparse_results = BM25_INDEX.search(query, k=N_RESULTS)
        
        # Combine results based on fusion method
        if USE_HYBRID and sparse_results:
            if FUSION_METHOD == "rrf":
                final_results = reciprocal_rank_fusion(dense_results, sparse_results, RRF_K)
            elif FUSION_METHOD == "weighted":
                final_results = weighted_fusion(dense_results, sparse_results, DENSE_WEIGHT, SPARSE_WEIGHT)
            elif FUSION_METHOD == "parallel":
                final_results = parallel_fusion(dense_results, sparse_results)
            else:
                # Fallback to dense only
                final_results = dense_results[:N_RESULTS]
        else:
            # Use dense results only
            final_results = dense_results[:N_RESULTS]
        
        # Format results for output
        content_results = []
        for doc in final_results:
            content = doc.page_content.strip()
            if content:
                # Add source information if available
                source = doc.metadata.get('source_file', 'Ngu·ªìn kh√¥ng x√°c ƒë·ªãnh')
                
                # Add retrieval information
                retrieval_info = []
                if 'fusion_score' in doc.metadata:
                    retrieval_info.append(f"ƒêi·ªÉm Fusion: {doc.metadata['fusion_score']:.3f}")
                if 'retrieval_method' in doc.metadata:
                    retrieval_info.append(f"Ph∆∞∆°ng ph√°p: {doc.metadata['retrieval_method']}")
                if 'bm25_score' in doc.metadata:
                    retrieval_info.append(f"ƒêi·ªÉm BM25: {doc.metadata['bm25_score']:.3f}")
                
                retrieval_str = f" ({', '.join(retrieval_info)})" if retrieval_info else ""
                content_results.append(f"Ngu·ªìn: {source}{retrieval_str}\nN·ªôi dung: {content}")
        
        if not content_results:
            return ["Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu"]
        
        return content_results
        
    except Exception as e:
        return [f"L·ªói t√¨m ki·∫øm c∆° s·ªü d·ªØ li·ªáu: {str(e)}"]


def search_vector_database(query: str) -> List[str]:
    """Legacy function for backward compatibility - now uses hybrid search"""
    return search_vector_database_hybrid(query)


def search_web(query: str) -> List[str]:
    """Search the web using DuckDuckGo through LangChain with comprehensive error handling"""
    import time
    import random
    
    try:
        if SEARCH_WRAPPER is None:
            return ["T√¨m ki·∫øm web kh√¥ng kh·∫£ d·ª•ng - tr√¨nh t√¨m ki·∫øm ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o"]
        
        # Add timeout handling with retry mechanism
        max_retries = 3
        base_delay = 1
        
        for attempt in range(max_retries):
            try:
                print(f"ƒêang th·ª≠ t√¨m ki·∫øm web (l·∫ßn {attempt + 1}/{max_retries}): {query}")
                
                # Use DuckDuckGo search with specified max_results
                search_results = SEARCH_WRAPPER.results(query, max_results=N_RESULTS)
                
                formatted_results = []
                
                for result in search_results:
                    # Extract information from each result dict
                    title = result.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')
                    snippet = result.get('snippet', 'Kh√¥ng c√≥ t√≥m t·∫Øt')
                    link = result.get('link', 'Kh√¥ng c√≥ li√™n k·∫øt')
                    
                    # Format the result for better readability
                    formatted_result = f"Ti√™u ƒë·ªÅ: {title}\nT√≥m t·∫Øt: {snippet}\nLi√™n k·∫øt: {link}"
                    formatted_results.append(formatted_result)
                
                if not formatted_results:
                    if attempt < max_retries - 1:
                        wait_time = base_delay * (attempt + 1) + random.randint(1, 3)
                        print(f"Kh√¥ng c√≥ k·∫øt qu·∫£, th·ª≠ l·∫°i sau {wait_time} gi√¢y...")
                        time.sleep(wait_time)
                        continue
                    else:
                        return [f"Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ t√¨m ki·∫øm web cho truy v·∫•n: '{query}' sau {max_retries} l·∫ßn th·ª≠"]
                
                print(f"T√¨m ki·∫øm web th√†nh c√¥ng: {len(formatted_results)} k·∫øt qu·∫£")
                return formatted_results
                
            except Exception as attempt_error:
                error_msg = str(attempt_error).lower()
                
                # Handle different types of errors
                if "ratelimit" in error_msg or "rate limit" in error_msg or "202" in str(attempt_error):
                    if attempt < max_retries - 1:
                        # Exponential backoff with jitter for rate limits
                        wait_time = (base_delay * (2 ** attempt)) + random.randint(0, 3)
                        print(f"‚ö†Ô∏è Rate limit detected (l·∫ßn {attempt + 1}): {attempt_error}")
                        print(f"Ch·ªù {wait_time} gi√¢y ƒë·ªÉ tr√°nh rate limit...")
                        time.sleep(wait_time)
                        continue
                    else:
                        return [
                            f"üö´ DuckDuckGo ƒë√£ gi·ªõi h·∫°n t·∫ßn su·∫•t truy c·∫≠p cho truy v·∫•n: '{query}'",
                            "ƒêi·ªÅu n√†y th∆∞·ªùng x·∫£y ra khi c√≥ qu√° nhi·ªÅu requests trong th·ªùi gian ng·∫Øn.",
                            "üí° Gi·∫£i ph√°p:",
                            "- H·ªá th·ªëng s·∫Ω d·ª±a v√†o c∆° s·ªü d·ªØ li·ªáu n·ªôi b·ªô",
                            "- Th·ª≠ l·∫°i sau v√†i ph√∫t",
                            "- Ho·∫∑c s·ª≠ d·ª•ng t·ª´ kh√≥a kh√°c ng·∫Øn g·ªçn h∆°n",
                            f"Chi ti·∫øt: {attempt_error}"
                        ]
                
                elif any(keyword in error_msg for keyword in ['timeout', 'timed out', 'connection', 'network']):
                    if attempt < max_retries - 1:
                        wait_time = base_delay * (attempt + 1) + random.randint(2, 5)
                        print(f"üåê L·ªói k·∫øt n·ªëi (l·∫ßn {attempt + 1}): {attempt_error}")
                        print(f"Th·ª≠ l·∫°i sau {wait_time} gi√¢y...")
                        time.sleep(wait_time)
                        continue
                    else:
                        return [
                            f"üåê L·ªói k·∫øt n·ªëi m·∫°ng cho truy v·∫•n: '{query}' sau {max_retries} l·∫ßn th·ª≠.",
                            "C√≥ th·ªÉ do:",
                            "- M·∫°ng internet kh√¥ng ·ªïn ƒë·ªãnh",
                            "- DuckDuckGo t·∫°m th·ªùi kh√¥ng kh·∫£ d·ª•ng",
                            "- Firewall/proxy ch·∫∑n k·∫øt n·ªëi",
                            "üí° H·ªá th·ªëng s·∫Ω ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu n·ªôi b·ªô."
                        ]
                
                elif "blocked" in error_msg or "forbidden" in error_msg or "403" in str(attempt_error):
                    return [
                        f"üö´ Truy c·∫≠p b·ªã ch·∫∑n cho truy v·∫•n: '{query}'",
                        "DuckDuckGo c√≥ th·ªÉ ƒë√£ ch·∫∑n IP n√†y do qu√° nhi·ªÅu requests.",
                        "üí° Gi·∫£i ph√°p:",
                        "- ƒê·ª£i 15-30 ph√∫t tr∆∞·ªõc khi th·ª≠ l·∫°i",
                        "- S·ª≠ d·ª•ng VPN n·∫øu c·∫ßn",
                        "- H·ªá th·ªëng s·∫Ω d·ª±a v√†o c∆° s·ªü d·ªØ li·ªáu n·ªôi b·ªô"
                    ]
                
                else:
                    # Other errors, don't retry immediately
                    if attempt < max_retries - 1:
                        wait_time = base_delay + random.randint(2, 5)
                        print(f"‚ùå L·ªói kh√°c (l·∫ßn {attempt + 1}): {attempt_error}")
                        print(f"Th·ª≠ l·∫°i sau {wait_time} gi√¢y...")
                        time.sleep(wait_time)
                        continue
                    else:
                        return [f"‚ùå L·ªói t√¨m ki·∫øm web cho '{query}': {attempt_error}"]
        
        # If we get here, all retries failed
        return [
            f"‚ùå Kh√¥ng th·ªÉ th·ª±c hi·ªán t√¨m ki·∫øm web cho '{query}' sau {max_retries} l·∫ßn th·ª≠.",
            "üí° H·ªá th·ªëng s·∫Ω ti·∫øp t·ª•c v·ªõi th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu n·ªôi b·ªô.",
            "Th·ª≠ l·∫°i sau √≠t ph√∫t ho·∫∑c s·ª≠ d·ª•ng t·ª´ kh√≥a kh√°c."
        ]
        
    except Exception as e:
        return [
            f"üí• L·ªói nghi√™m tr·ªçng khi t√¨m ki·∫øm web cho '{query}': {str(e)}",
            "üîÑ H·ªá th·ªëng s·∫Ω ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu n·ªôi b·ªô."
        ]


def observation_node(state: OverallState) -> Dict[str, Any]:
    """
    Execute database and web search queries to collect information
    This node does not use LLM - it only executes searches
    """
    import time
    import random
    
    database_queries = state.get("current_database_queries", [])
    web_queries = state.get("current_web_queries", [])
    
    query_results = []
    
    # Execute database queries (now using hybrid search)
    for query in database_queries:
        if query.strip():  # Skip empty queries
            try:
                results = search_vector_database_hybrid(query)
                query_results.append(QueryResult(
                    query=query,
                    results=results,
                    source="database"
                ))
            except Exception as e:
                query_results.append(QueryResult(
                    query=query,
                    results=[f"Truy v·∫•n c∆° s·ªü d·ªØ li·ªáu th·∫•t b·∫°i: {str(e)}"],
                    source="database"
                ))
    
    # Execute web search queries with rate limit awareness
    web_search_count = 0
    max_web_searches = 3  # Limit concurrent web searches
    
    for query in web_queries:
        if query.strip():  # Skip empty queries
            web_search_count += 1
            
            # Add delay between web searches to avoid rate limits
            if web_search_count > 1:
                delay = random.uniform(3, 8)  # Random delay 3-8 seconds
                print(f"‚è±Ô∏è Ch·ªù {delay:.1f}s gi·ªØa c√°c web search ƒë·ªÉ tr√°nh rate limit...")
                time.sleep(delay)
            
            # Skip excessive web searches to prevent rate limiting
            if web_search_count > max_web_searches:
                query_results.append(QueryResult(
                    query=query,
                    results=[
                        f"‚è≠Ô∏è B·ªè qua web search cho '{query}' ƒë·ªÉ tr√°nh rate limit",
                        f"ƒê√£ th·ª±c hi·ªán {max_web_searches} web searches, ∆∞u ti√™n c∆° s·ªü d·ªØ li·ªáu n·ªôi b·ªô",
                        "üí° Th√¥ng tin t·ª´ database th∆∞·ªùng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi"
                    ],
                    source="web"
                ))
                continue
            
            try:
                results = search_web(query)
                query_results.append(QueryResult(
                    query=query,
                    results=results,
                    source="web"
                ))
            except Exception as e:
                error_msg = f"T√¨m ki·∫øm web th·∫•t b·∫°i: {str(e)}"
                if any(keyword in str(e).lower() for keyword in ["ratelimit", "rate limit", "202", "blocked", "forbidden"]):
                    error_msg = f"üö´ Web search b·ªã gi·ªõi h·∫°n: {query}. Ch·ªâ s·ª≠ d·ª•ng k·∫øt qu·∫£ t·ª´ c∆° s·ªü d·ªØ li·ªáu."
                elif "timeout" in str(e).lower() or "timed out" in str(e).lower():
                    error_msg = f"‚è±Ô∏è Web search b·ªã timeout: {query}. Ch·ªâ s·ª≠ d·ª•ng k·∫øt qu·∫£ t·ª´ c∆° s·ªü d·ªØ li·ªáu."
                
                query_results.append(QueryResult(
                    query=query,
                    results=[error_msg],
                    source="web"
                ))

    print("\n[QUAN S√ÅT] ƒêang th·ª±c hi·ªán truy v·∫•n c∆° s·ªü d·ªØ li·ªáu v√† t√¨m ki·∫øm web...")
    
    # Print retrieval method info
    if USE_HYBRID and BM25_INDEX and BM25_INDEX.bm25_index:
        print(f"üîç S·ª≠ d·ª•ng t√¨m ki·∫øm hybrid: fusion {FUSION_METHOD} (Dense: {DENSE_WEIGHT}, Sparse: {SPARSE_WEIGHT})")
    else:
        print("üîç Ch·ªâ s·ª≠ d·ª•ng t√¨m ki·∫øm dense")

    # Count successful vs failed searches
    db_success = sum(1 for r in query_results if r.source == "database" and not any("th·∫•t b·∫°i" in res for res in r.results))
    web_success = sum(1 for r in query_results if r.source == "web" and not any(keyword in res for res in r.results for keyword in ["th·∫•t b·∫°i", "timeout", "rate limit", "L·ªói", "üö´", "‚è±Ô∏è", "‚ùå"]))
    
    print(f"üìä K·∫øt qu·∫£: {len(query_results)} truy v·∫•n (DB: {db_success}/{len(database_queries)} ‚úÖ, Web: {web_success}/{len(web_queries)} ‚úÖ)")
    
    for i, result in enumerate(query_results, 1):
        source_vietnamese = "C∆† S·ªû D·ªÆ LI·ªÜU" if result.source.upper() == "DATABASE" else "WEB"
        
        # Determine status based on result content
        has_error = any(keyword in str(result.results).lower() for keyword in ["th·∫•t b·∫°i", "timeout", "rate limit", "l·ªói", "üö´", "‚è±Ô∏è", "‚ùå", "‚è≠Ô∏è"])
        status = "‚ö†Ô∏è" if has_error else "‚úÖ"
        
        print(f"{i}. {status} TRUY V·∫§N {source_vietnamese}: {result.query}")
        for j, content in enumerate(result.results, 1):
            # Truncate long content for display
            display_content = content[:200] + "..." if len(content) > 200 else content
            print(f"   {j}. {display_content}")
    
    return {
        "current_query_results": query_results
    }